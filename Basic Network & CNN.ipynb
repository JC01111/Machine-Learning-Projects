{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q4.1\n",
    "Implementation of `activations.ReLU`:\n",
    "\n",
    "```python\n",
    "class ReLU(Activation):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, Z: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Forward pass for relu activation:\n",
    "        f(z) = z if z >= 0\n",
    "               0 otherwise\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        Z  input pre-activations (any shape)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        f(z) as described above applied elementwise to `Z`\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE ###\n",
    "        return np.maximum(0, Z)\n",
    "\n",
    "    def backward(self, Z: np.ndarray, dY: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Backward pass for relu activation.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        Z   input to `forward` method\n",
    "        dY  gradient of loss w.r.t. the output of this layer\n",
    "            same shape as `Z`\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        gradient of loss w.r.t. input of this layer\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE ###\n",
    "        dZ = np.array(Z, copy=True)\n",
    "        dZ[Z<0], dZ[Z>=0] = 0, 1\n",
    "        return dY * dZ\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q4.2\n",
    "Implementation of `layers.FullyConnected`:\n",
    "\n",
    "```python\n",
    "class FullyConnected(Layer):\n",
    "    \"\"\"A fully-connected layer multiplies its input by a weight matrix, adds\n",
    "    a bias, and then applies an activation function.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, n_out: int, activation: str, weight_init=\"xavier_uniform\"\n",
    "    ) -> None:\n",
    "\n",
    "        super().__init__()\n",
    "        self.n_in = None\n",
    "        self.n_out = n_out\n",
    "        self.activation = initialize_activation(activation)\n",
    "\n",
    "        # instantiate the weight initializer\n",
    "        self.init_weights = initialize_weights(weight_init, activation=activation)\n",
    "\n",
    "    def _init_parameters(self, X_shape: Tuple[int, int]) -> None:\n",
    "        \"\"\"Initialize all layer parameters (weights, biases).\"\"\"\n",
    "        self.n_in = X_shape[1]\n",
    "\n",
    "        ### BEGIN YOUR CODE ###\n",
    "\n",
    "        W = self.init_weights((self.n_in, self.n_out))  # Takes the shape of the design matrix\n",
    "        b = np.zeros((1, self.n_out))  # Initialize bias to zero, 1xn^(l+1)\n",
    "        Z = []\n",
    "        X = []\n",
    "\n",
    "        self.parameters = OrderedDict({\"W\": W, \"b\": b}) # DO NOT CHANGE THE KEYS\n",
    "        self.cache = OrderedDict({\"Z\": Z, \"X\": X})  # cache for backprop\n",
    "        self.gradients = OrderedDict({\"W\": np.zeros_like(W), \"b\": np.zeros_like(b)})  # parameter gradients initialized to zero\n",
    "                                                                            # MUST HAVE THE SAME KEYS AS `self.parameters`\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Forward pass: multiply by a weight matrix, add a bias, apply activation.\n",
    "        Also, store all necessary intermediate results in the `cache` dictionary\n",
    "        to be able to compute the backward pass.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X  input matrix of shape (batch_size, input_dim)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        a matrix of shape (batch_size, output_dim)\n",
    "        \"\"\"\n",
    "        # initialize layer parameters if they have not been initialized\n",
    "        if self.n_in is None:\n",
    "            self._init_parameters(X.shape)\n",
    "\n",
    "        ### BEGIN YOUR CODE ###\n",
    "        W, b = self.parameters[\"W\"], self.parameters[\"b\"]\n",
    "        # perform an affine transformation and activation\n",
    "        Z = X @ W + b\n",
    "        out = self.activation(Z)\n",
    "        # store information necessary for backprop in `self.cache`\n",
    "        self.cache[\"Z\"], self.cache[\"X\"] = Z, X\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dLdY: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Backward pass for fully connected layer.\n",
    "        Compute the gradients of the loss with respect to:\n",
    "            1. the weights of this layer (mutate the `gradients` dictionary)\n",
    "            2. the bias of this layer (mutate the `gradients` dictionary)\n",
    "            3. the input of this layer (return this)\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dLdY  gradient of the loss with respect to the output of this layer\n",
    "              shape (batch_size, output_dim)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        gradient of the loss with respect to the input of this layer\n",
    "        shape (batch_size, input_dim)\n",
    "        \"\"\"\n",
    "        ### BEGIN YOUR CODE ###\n",
    "\n",
    "        # unpack the cache\n",
    "        W, b = self.parameters[\"W\"], self.parameters[\"b\"]\n",
    "        Z, X = self.cache[\"Z\"], self.cache[\"X\"]\n",
    "        # compute the gradients of the loss w.r.t. all parameters as well as the\n",
    "        # input of the layer\n",
    "        dLdZ = self.activation.backward(Z, dLdY)\n",
    "        dLdW = X.T @ dLdZ\n",
    "        dX = dLdZ @ W.T\n",
    "        dLdb = np.sum(dLdZ, axis=0, keepdims=True)\n",
    "        # store the gradients in `self.gradients`\n",
    "        # the gradient for self.parameters[\"W\"] should be stored in\n",
    "        # self.gradients[\"W\"], etc.\n",
    "        self.gradients[\"W\"], self.gradients[\"b\"] = dLdW, dLdb\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "        return dX\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q4.3\n",
    "\n",
    "Implementation of `activations.SoftMax`:\n",
    "\n",
    "```python\n",
    "class SoftMax(Activation):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, Z: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Forward pass for softmax activation.\n",
    "        Hint: The naive implementation might not be numerically stable.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        Z  input pre-activations (any shape)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        f(z) as described above applied elementwise to `Z`\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE ###\n",
    "        shifted_Z = Z - np.max(Z, axis=-1, keepdims=True)\n",
    "        exp = np.exp(shifted_Z)\n",
    "        return exp / np.sum(exp, axis=-1, keepdims=True)\n",
    "\n",
    "    def backward(self, Z: np.ndarray, dY: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Backward pass for softmax activation.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        Z   input to `forward` method\n",
    "        dY  gradient of loss w.r.t. the output of this layer\n",
    "            same shape as `Z`\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        gradient of loss w.r.t. input of this layer\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE ###\n",
    "        softmax = self.forward(Z)\n",
    "        dZ = dY - np.sum(dY * softmax, axis=-1, keepdims=True)\n",
    "        return dZ * softmax\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q4.4\n",
    "Implementation of `losses.CrossEntropy`:\n",
    "\n",
    "```python\n",
    "class CrossEntropy(Loss):\n",
    "    \"\"\"Cross entropy loss function.\"\"\"\n",
    "\n",
    "    def __init__(self, name: str) -> None:\n",
    "        self.name = name\n",
    "\n",
    "    def __call__(self, Y: np.ndarray, Y_hat: np.ndarray) -> float:\n",
    "        return self.forward(Y, Y_hat)\n",
    "\n",
    "    def forward(self, Y: np.ndarray, Y_hat: np.ndarray) -> float:\n",
    "        \"\"\"Computes the loss for predictions `Y_hat` given one-hot encoded labels\n",
    "        `Y`.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        Y      one-hot encoded labels of shape (batch_size, num_classes)\n",
    "        Y_hat  model predictions in range (0, 1) of shape (batch_size, num_classes)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        a single float representing the loss\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE ###\n",
    "        return -(np.sum(Y * np.log(Y_hat))) / Y.shape[0]\n",
    "\n",
    "    def backward(self, Y: np.ndarray, Y_hat: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Backward pass of cross-entropy loss.\n",
    "        NOTE: This is correct ONLY when the loss function is SoftMax.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        Y      one-hot encoded labels of shape (batch_size, num_classes)\n",
    "        Y_hat  model predictions in range (0, 1) of shape (batch_size, num_classes)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        the gradient of the cross-entropy loss with respect to the vector of\n",
    "        predictions, `Y_hat`\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE ###\n",
    "        return -((Y/Y_hat)/Y.shape[0])\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q5\n",
    "\n",
    "Implementation of `models.NeuralNetwork.forward`:\n",
    "\n",
    "```python\n",
    "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"One forward pass through all the layers of the neural network.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X  design matrix whose must match the input shape required by the\n",
    "           first layer\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        forward pass output, matches the shape of the output of the last layer\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE ###\n",
    "        # Iterate through the network's layers.\n",
    "        for layer in self.layers:\n",
    "            X = layer.forward(X)\n",
    "        return X\n",
    "\n",
    "```\n",
    "<br>\n",
    "\n",
    "Implementation of `models.NeuralNetwork.backward`:\n",
    "\n",
    "```python\n",
    "    def backward(self, target: np.ndarray, out: np.ndarray) -> float:\n",
    "        \"\"\"One backward pass through all the layers of the neural network.\n",
    "        During this phase we calculate the gradients of the loss with respect to\n",
    "        each of the parameters of the entire neural network. Most of the heavy\n",
    "        lifting is done by the `backward` methods of the layers, so this method\n",
    "        should be relatively simple. Also make sure to compute the loss in this\n",
    "        method and NOT in `self.forward`.\n",
    "\n",
    "        Note: Both input arrays have the same shape.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        target  the targets we are trying to fit to (e.g., training labels)\n",
    "        out     the predictions of the model on training data\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        the loss of the model given the training inputs and targets\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE ###\n",
    "        # Compute the loss.\n",
    "        # Backpropagate through the network's layers.\n",
    "        loss = self.loss.forward(target, out)   # Y, Y_hat\n",
    "        dLdY = self.loss.backward(target, out)\n",
    "        for layer in reversed(self.layers):   # Backpropagate\n",
    "            dLdY = layer.backward(dLdY)\n",
    "        return loss\n",
    "\n",
    "```\n",
    "<br>\n",
    "\n",
    "Implementation of `models.NeuralNetwork.predict`:\n",
    "\n",
    "```python\n",
    "    def predict(self, X: np.ndarray, Y: np.ndarray) -> Tuple[np.ndarray, float]:\n",
    "        \"\"\"Make a forward and backward pass to calculate the predictions and\n",
    "        loss of the neural network on the given data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X  input features\n",
    "        Y  targets (same length as `X`)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        a tuple of the prediction and loss\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE ###\n",
    "        # Do a forward pass. Maybe use a function you already wrote?\n",
    "        # Get the loss. Remember that the `backward` function returns the loss.\n",
    "        Y_hat = self.forward(X)\n",
    "        loss = self.backward(Y, Y_hat)\n",
    "        return Y_hat, loss\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "4.2998752849492583e-16\n",
      "6.955527313080394e-16\n"
     ]
    }
   ],
   "source": [
    "# Q6.1\n",
    "import numpy as np\n",
    "np.random.seed(189)\n",
    "\n",
    "# 1\n",
    "A = np.random.rand(5, 5)\n",
    "einsum_trace = np.einsum('ii', A)\n",
    "trace = np.trace(A)\n",
    "print(np.linalg.norm(einsum_trace - trace))\n",
    "\n",
    "# 2\n",
    "A = np.random.rand(5, 5)\n",
    "B = np.random.rand(5, 5)\n",
    "einsum_matrix_product = np.einsum('ij, jk->ik', A, B)\n",
    "matrix_product = A @ B\n",
    "print(np.linalg.norm(einsum_matrix_product - matrix_product))\n",
    "\n",
    "# 3\n",
    "C, D = np.random.rand(3, 4, 5), np.random.rand(3, 5, 6)\n",
    "einsum_product = np.einsum('ijk, ikl->ijl', C, D)\n",
    "matrix_mul = np.matmul(C, D)\n",
    "print(np.linalg.norm(einsum_product - matrix_mul))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q6.2\n",
    "\n",
    "Implementation of `layers.Conv2D.forward`:\n",
    "\n",
    "```python\n",
    "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Forward pass for convolutional layer. This layer convolves the input\n",
    "        `X` with a filter of weights, adds a bias term, and applies an activation\n",
    "        function to compute the output. This layer also supports padding and\n",
    "        integer strides. Intermediates necessary for the backward pass are stored\n",
    "        in the cache.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X  input with shape (batch_size, in_rows, in_cols, in_channels)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        output feature maps with shape (batch_size, out_rows, out_cols, out_channels)\n",
    "        \"\"\"\n",
    "        if self.n_in is None:\n",
    "            self._init_parameters(X.shape)\n",
    "\n",
    "        W = self.parameters[\"W\"]\n",
    "        b = self.parameters[\"b\"]\n",
    "\n",
    "        kernel_height, kernel_width, in_channels, out_channels = W.shape\n",
    "        n_examples, in_rows, in_cols, in_channels = X.shape\n",
    "        kernel_shape = (kernel_height, kernel_width)\n",
    "\n",
    "        ### BEGIN YOUR CODE ###\n",
    "        pad_h, pad_w = self.pad\n",
    "        out_rows = (in_rows + 2*pad_h - kernel_height) // self.stride + 1 # formula from disc\n",
    "        out_cols = (in_cols + 2*pad_w - kernel_width) // self.stride + 1  # formula from disc\n",
    "        # Padding for four axis\n",
    "        X_pad = np.pad(X, pad_width=((0, 0), (pad_h, pad_h), (pad_w, pad_w), (0, 0)), mode='constant')\n",
    "        # implement a convolutional forward pass\n",
    "        Z = np.empty((n_examples, out_rows, out_cols, out_channels), dtype=X.dtype)\n",
    "        for i in range(out_rows):\n",
    "            for j in range(out_cols):\n",
    "                i_start, j_start = i*self.stride, j*self.stride\n",
    "                i_end, j_end = i_start+kernel_height, j_start+kernel_width\n",
    "                X_slice = X_pad[:,i_start:i_end,j_start:j_end,:]\n",
    "                np.einsum('ijcn, bijc->bn', W, X_slice, out=(Z[:,i,j,:]))\n",
    "        # X_pad: (16, 18, 18, 32)    W: (3, 3, 3, 32)     b: (1, 32)     Z: (16, 16, 16, 32)\n",
    "        Z += b\n",
    "        out = self.activation.forward(Z)\n",
    "        # cache any values required for backprop\n",
    "        self.cache['Z'], self.cache['X'] = Z, X\n",
    "        self.cache['X_pad'] = X_pad\n",
    "        self.cache['out_rows'], self.cache['out_cols'] = out_rows, out_cols\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "        return out\n",
    "\n",
    "```\n",
    "<br>\n",
    "\n",
    "Implementation of `layers.Conv2D.backward`:\n",
    "\n",
    "```python\n",
    "    def backward(self, dLdY: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Backward pass for conv layer. Computes the gradients of the output\n",
    "        with respect to the input feature maps as well as the filter weights and\n",
    "        biases.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dLdY  gradient of loss with respect to output of this layer\n",
    "              shape (batch_size, out_rows, out_cols, out_channels)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        gradient of the loss with respect to the input of this layer\n",
    "        shape (batch_size, in_rows, in_cols, in_channels)\n",
    "        \"\"\"\n",
    "        ### BEGIN YOUR CODE ###\n",
    "        W, b = self.parameters[\"W\"], self.parameters[\"b\"]\n",
    "        Z, X = self.cache['Z'], self.cache['X']\n",
    "        pad_h, pad_w = self.pad\n",
    "        X_pad = self.cache['X_pad']\n",
    "        out_rows, out_cols = self.cache['out_rows'], self.cache['out_cols']\n",
    "        # perform a backward pass\n",
    "        dLdZ = self.activation.backward(Z, dLdY)\n",
    "        dX = np.zeros_like(X_pad)\n",
    "\n",
    "        for i in range(out_rows):\n",
    "            for j in range(out_cols):\n",
    "                i_start, j_start = i*self.stride, j*self.stride\n",
    "                i_end, j_end = i_start+W.shape[0], j_start+W.shape[1]\n",
    "                X_slice = X_pad[:,i_start:i_end,j_start:j_end,:]\n",
    "                self.gradients['W'] += np.einsum('bn, bijc->ijcn', dLdZ[:,i,j,:], X_slice)\n",
    "                dX[:,i_start:i_end,j_start:j_end,:] += np.einsum('bn, ijcn->bijc', dLdZ[:,i,j,:], W)\n",
    "\n",
    "        self.gradients['b'] = np.sum(dLdZ, axis=(0, 1, 2)).reshape(1, -1)\n",
    "        dX = dX[:, pad_h:pad_h+X.shape[1], pad_w:pad_w+X.shape[2],:]    # Remove padding\n",
    "        ### END YOUR CODE ###\n",
    "        \n",
    "        return dX\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q6.3\n",
    "\n",
    "Implementation of `layers.Pool2D.forward`:\n",
    "\n",
    "```python\n",
    "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Forward pass: use the pooling function to aggregate local information\n",
    "        in the input. This layer typically reduces the spatial dimensionality of\n",
    "        the input while keeping the number of feature maps the same.\n",
    "\n",
    "        As with all other layers, please make sure to cache the appropriate\n",
    "        information for the backward pass.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X  input array of shape (batch_size, in_rows, in_cols, channels)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pooled array of shape (batch_size, out_rows, out_cols, channels)\n",
    "        \"\"\"\n",
    "        ### BEGIN YOUR CODE ###\n",
    "        kernel_height, kernel_width = self.kernel_shape\n",
    "        batch_size, in_rows, in_cols, channels = X.shape\n",
    "        pad_h, pad_w = self.pad\n",
    "        out_rows = (in_rows + 2*pad_h - kernel_height) // self.stride + 1\n",
    "        out_cols = (in_cols + 2*pad_w - kernel_width) // self.stride + 1 \n",
    "\n",
    "        X_pad = np.pad(X, pad_width=((0, 0), (pad_h, pad_h), (pad_w, pad_w), (0, 0)), mode='constant')\n",
    "        X_pool = np.zeros((batch_size, out_rows, out_cols, channels))\n",
    "        # implement the forward pass\n",
    "        for i in range(out_rows):\n",
    "            for j in range(out_cols):\n",
    "                i_start, j_start = i*self.stride, j*self.stride\n",
    "                i_end, j_end = i_start+kernel_height, j_start+kernel_width\n",
    "                X_pool[:,i,j,:] = self.pool_fn(X_pad[:,i_start:i_end,j_start:j_end,:], axis=(1, 2))\n",
    "        # cache any values required for backpro\n",
    "        self.cache['X_pad'], self.cache['X_shape'] = X_pad, X.shape\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "        return X_pool\n",
    "```\n",
    "<br>\n",
    "\n",
    "Implementation of `layers.Pool2D.backward`:\n",
    "\n",
    "```python\n",
    "    def backward(self, dLdY: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Backward pass for pooling layer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dLdY  gradient of loss with respect to the output of this layer\n",
    "              shape (batch_size, out_rows, out_cols, channels)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        gradient of loss with respect to the input of this layer\n",
    "        shape (batch_size, in_rows, in_cols, channels)\n",
    "        \"\"\"\n",
    "        ### BEGIN YOUR CODE ###\n",
    "        ker_h, ker_w = self.kernel_shape\n",
    "        X_pad = self.cache['X_pad']\n",
    "        batch_size, in_rows, in_cols, channels = self.cache['X_shape']\n",
    "        batch_size, out_rows, out_cols, channels = dLdY.shape\n",
    "        pad_h, pad_w = self.pad\n",
    "        dX = np.zeros_like(X_pad)\n",
    "        # perform a backward pass\n",
    "        for i in range(out_rows):\n",
    "            for j in range(out_cols):\n",
    "                i_start, j_start = i*self.stride, j*self.stride\n",
    "                i_end, j_end = i_start+ker_h, j_start+ker_w\n",
    "                if self.mode == 'max':\n",
    "                    X_i = X_pad[:,i_start:i_end,j_start:j_end,:]    # Partition of X_pad\n",
    "                    X_i_flat = X_i.reshape(batch_size, ker_h*ker_w, channels)   # Flatten spatial dimensions only\n",
    "                    X_i_indices = self.arg_pool_fn(X_i_flat, axis=1)    # Find the argmax of the flatten spatial dimensions, (24, 8)\n",
    "                    batch_idx, channel_idx = np.indices((batch_size, channels))\n",
    "                    # Create a mask as same shape as X_i_flat\n",
    "                    mask = np.zeros_like(X_i_flat)\n",
    "                    mask[batch_idx, X_i_indices, channel_idx] = 1     # Set max position to be 1\n",
    "                    mask = mask.reshape(batch_size, ker_h, ker_w, channels)    # Reshape back to mask size\n",
    "                    dX[:,i_start:i_end,j_start:j_end,:] += mask * dLdY[:,i:i+1,j:j+1,:]\n",
    "                else:\n",
    "                    dX[:,i_start:i_end,j_start:j_end,:] += dLdY[:,i:i+1,j:j+1,:] / (ker_h*ker_w)    # 1/n(mask area)\n",
    "\n",
    "        dX = dX[:, pad_h:pad_h+in_rows, pad_w:pad_w+in_cols, :]    # Remove padding\n",
    "        ### END YOUR CODE ### \n",
    "\n",
    "        return dX\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Function Implementations:\n",
    "\n",
    "Implementation of `activations.Linear`:\n",
    "\n",
    "```python\n",
    "class Linear(Activation):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, Z: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Forward pass for f(z) = z.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        Z  input pre-activations (any shape)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        f(z) as described above applied elementwise to `Z`\n",
    "        \"\"\"\n",
    "        return Z\n",
    "\n",
    "    def backward(self, Z: np.ndarray, dY: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Backward pass for f(z) = z.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        Z   input to `forward` method\n",
    "        dY  gradient of loss w.r.t. the output of this layer\n",
    "            same shape as `Z`\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        gradient of loss w.r.t. input of this layer\n",
    "        \"\"\"\n",
    "        return dY\n",
    "\n",
    "```\n",
    "<br>\n",
    "\n",
    "Implementation of `activations.Sigmoid`:\n",
    "\n",
    "```python\n",
    "class Sigmoid(Activation):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, Z: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Forward pass for sigmoid function:\n",
    "        f(z) = 1 / (1 + exp(-z))\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        Z  input pre-activations (any shape)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        f(z) as described above applied elementwise to `Z`\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE ###\n",
    "        return 1 / (1 + np.exp(-Z))\n",
    "\n",
    "    def backward(self, Z: np.ndarray, dY: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Backward pass for sigmoid.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        Z   input to `forward` method\n",
    "        dY  gradient of loss w.r.t. the output of this layer\n",
    "            same shape as `Z`\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        gradient of loss w.r.t. input of this layer\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE ###\n",
    "        sigmoid = self.forward(Z)\n",
    "        return dY * (sigmoid * (1-sigmoid))\n",
    "\n",
    "```\n",
    "<br>\n",
    "\n",
    "Implementation of `activations.ReLU`:\n",
    "\n",
    "```python\n",
    "class ReLU(Activation):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, Z: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Forward pass for relu activation:\n",
    "        f(z) = z if z >= 0\n",
    "               0 otherwise\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        Z  input pre-activations (any shape)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        f(z) as described above applied elementwise to `Z`\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE ###\n",
    "        return np.maximum(0, Z)\n",
    "\n",
    "    def backward(self, Z: np.ndarray, dY: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Backward pass for relu activation.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        Z   input to `forward` method\n",
    "        dY  gradient of loss w.r.t. the output of this layer\n",
    "            same shape as `Z`\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        gradient of loss w.r.t. input of this layer\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE ###\n",
    "        dZ = np.array(Z, copy=True)\n",
    "        dZ[Z<0], dZ[Z>=0] = 0, 1\n",
    "        return dY * dZ\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
