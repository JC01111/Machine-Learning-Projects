{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f8b8c00b-d03d-474e-af4f-39478e9ed35d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing the titanic dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3j/yt011p3d543_vhy83qz20ctc0000gr/T/ipykernel_56996/2462536374.py:61: VisibleDeprecationWarning: Reading unicode strings without specifying the encoding argument is deprecated. Set the encoding, use None for the system default.\n",
      "  data = genfromtxt('datasets/titanic/titanic_training.csv', delimiter=',', dtype=None)\n",
      "/var/folders/3j/yt011p3d543_vhy83qz20ctc0000gr/T/ipykernel_56996/2462536374.py:62: VisibleDeprecationWarning: Reading unicode strings without specifying the encoding argument is deprecated. Set the encoding, use None for the system default.\n",
      "  test_data = genfromtxt('datasets/titanic/titanic_testing_data.csv', delimiter=',', dtype=None)\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import scipy.io\n",
    "from scipy.stats import mode\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, VotingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from pydot import graph_from_dot_data\n",
    "import io\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "eps = 1e-5  # a small number\n",
    "\n",
    "# Dataset\n",
    "spam = scipy.io.loadmat('datasets/spam_data/spam_data.mat')\n",
    "spam_training_data, spam_training_labels = spam['training_data'], np.squeeze(spam['training_labels'])\n",
    "spam_test = spam['test_data']\n",
    "\n",
    "\n",
    "# Preprocess for titanic data\n",
    "def preprocess(data, fill_mode=True, min_freq=10, onehot_cols=[]):\n",
    "    # fill_mode = False\n",
    "\n",
    "    # Temporarily assign -1 to missing data\n",
    "    data[data == b''] = '-1'\n",
    "    \n",
    "    # Hash the columns (used for handling strings)\n",
    "    onehot_encoding = []\n",
    "    onehot_features = []\n",
    "    for col in onehot_cols:\n",
    "        counter = Counter(data[:, col])\n",
    "        for term in counter.most_common():\n",
    "            if term[0] == b'-1':\n",
    "                continue\n",
    "            if term[-1] <= min_freq:\n",
    "                break\n",
    "            onehot_features.append(term[0])\n",
    "            onehot_encoding.append((data[:, col] == term[0]).astype(float))\n",
    "        data[:, col] = '0'\n",
    "    onehot_encoding = np.array(onehot_encoding).T\n",
    "    data = np.hstack(\n",
    "        [np.array(data, dtype=float),\n",
    "         np.array(onehot_encoding)])\n",
    "    # Replace missing data with the mode value.\n",
    "    if fill_mode:\n",
    "        # TODO\n",
    "        for col in range(data.shape[1]):\n",
    "            missing_idx = (data[:, col] == -1)\n",
    "            if missing_idx.any():\n",
    "                col_mode = mode(data[~missing_idx, col])[0]\n",
    "                data[missing_idx, col] = col_mode\n",
    "\n",
    "    return data, onehot_features\n",
    "\n",
    "\n",
    "# Load titanic data\n",
    "data = genfromtxt('datasets/titanic/titanic_training.csv', delimiter=',', dtype=None)\n",
    "test_data = genfromtxt('datasets/titanic/titanic_testing_data.csv', delimiter=',', dtype=None)\n",
    "y = data[1:, 0]  # label = survived\n",
    "class_names = [\"Died\", \"Survived\"]\n",
    "\n",
    "labeled_idx = np.where(y != b'')[0]\n",
    "y = np.array(y[labeled_idx], dtype=float).astype(int)\n",
    "print(\"Preprocessing the titanic dataset\")\n",
    "X, onehot_features = preprocess(data[1:, 1:], onehot_cols=[1, 5, 7, 8])  # onehot_cols: [pclass, parch, fare, cabin]\n",
    "X = X[labeled_idx, :]\n",
    "Z, _ = preprocess(test_data[1:, :], onehot_cols=[1, 5, 7, 8])\n",
    "assert X.shape[1] == Z.shape[1]\n",
    "features = list(data[0, 1:]) + onehot_features\n",
    "\n",
    "# Rename titanic data\n",
    "titanic_training, titanic_training_labels = X, y\n",
    "titanic_test = Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a08a558-d9bb-4336-8b2b-aad2d1ac5f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper func\n",
    "def evaluate(clf):\n",
    "    print(\"Cross validation\", cross_val_score(clf, X, y))\n",
    "    if hasattr(clf, \"decision_trees\"):\n",
    "        counter = Counter([t.tree_.feature[0] for t in clf.decision_trees])\n",
    "        first_splits = [\n",
    "            (features[term[0]], term[1]) for term in counter.most_common()\n",
    "        ]\n",
    "        print(\"First splits\", first_splits)\n",
    "\n",
    "def evaluate_simple(pred, y):\n",
    "    return np.mean(pred == y)\n",
    "\n",
    "def train_valid_split(X, y, holdout):\n",
    "    num = X.shape[0]\n",
    "    split = int(num * holdout)\n",
    "    X_train, X_valid = X[:split], X[split:]\n",
    "    y_train, y_valid = y[:split], y[split:]    \n",
    "    return X_train, X_valid, y_train, y_valid\n",
    "\n",
    "def results_to_csv(y_test, name, method):\n",
    "    y_test = y_test.astype(int)\n",
    "    df = pd.DataFrame({'Category': y_test})\n",
    "    df.index += 1 # Ensures that the index starts at 1\n",
    "    df.to_csv(f'{name}_{method}_pred.csv', index_label='Id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4182c386-1c08-4973-89c7-358037852297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spam Decision Tree Accuracy: 0.7937466690353526\n",
      "Titanic Decision Tree Accuracy: 0.8126858275520317\n",
      "\n",
      "Spam training accuracy for decisionTree: 0.8071111111111111,\n",
      "Spam validation accuracy for decisionTree: 0.7708703374777975\n",
      "\n",
      "Titanic training accuracy for decisionTree: 0.8208955223880597,\n",
      "Titanic validation accuracy for decisionTree: 0.7178217821782178\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Decision Trees\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=3, feature_labels=None):\n",
    "        self.max_depth = max_depth\n",
    "        self.features = feature_labels\n",
    "        self.left, self.right = None, None  # for non-leaf nodes\n",
    "        self.split_idx, self.thresh = None, None  # for non-leaf nodes\n",
    "        self.data, self.pred = None, None  # for leaf nodes\n",
    "        \n",
    "    @staticmethod\n",
    "    def entropy(y):\n",
    "        # TODO\n",
    "        base_probabilities = []\n",
    "        for class_label in np.unique(y):\n",
    "            count = len(y[np.where(y==class_label)])\n",
    "            base_probabilities.append(float(count / len(y)))\n",
    "\n",
    "        H_S = -1* sum([p_c * np.log2(p_c) for p_c in base_probabilities])\n",
    "        return H_S\n",
    "\n",
    "    @staticmethod\n",
    "    def information_gain(X, y, idx, thresh):\n",
    "        # TODO\n",
    "        H_S = DecisionTree.entropy(y)\n",
    "        left_indices, right_indices = np.where(X[:, idx] < thresh)[0], np.where(X[:, idx] >= thresh)[0] \n",
    "        yl, yr = y[left_indices], y[right_indices]\n",
    "        Sl, Sr = len(yl) / len(y), len(yr) / len(y)\n",
    "        H_after = (Sl * DecisionTree.entropy(yl) + Sr * DecisionTree.entropy(yr) ) / (Sl + Sr)\n",
    "        return H_S - H_after\n",
    "    \n",
    "    @staticmethod\n",
    "    def gini_impurity(X, y, thresh):\n",
    "        # TODO\n",
    "        _, counts = np.unique(y, return_counts=True)\n",
    "        prob = counts / counts.sum()\n",
    "        return 1 - np.sum(prob**2)\n",
    "\n",
    "    @staticmethod\n",
    "    def gini_purification(X, y, thresh):\n",
    "        # TODO\n",
    "        init_gini = self.gini_impurity(X, y, thresh)\n",
    "        left_indices, right_indices = X[:, self.split_idx] < thresh, X[:, self.split_idx] >= thresh\n",
    "        yl, yr = y[left_indices], y[right_indices]\n",
    "        Sl, Sr = len(yl) / len(y), len(yr) / len(y)\n",
    "        after_gini = (Sl * yl + Sr * yr) / (Sl + Sr)\n",
    "        return init_gini - after_gini\n",
    "\n",
    "    def split(self, X, y, idx, thresh):\n",
    "        X0, idx0, X1, idx1 = self.split_test(X, idx=idx, thresh=thresh)\n",
    "        y0, y1 = y[idx0], y[idx1]\n",
    "        return X0, y0, X1, y1\n",
    "\n",
    "    def split_test(self, X, idx, thresh):\n",
    "        idx0 = np.where(X[:, idx] < thresh)[0]\n",
    "        idx1 = np.where(X[:, idx] >= thresh)[0]\n",
    "        X0, X1 = X[idx0, :], X[idx1, :]\n",
    "        return X0, idx0, X1, idx1\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # TODO\n",
    "        y = y.astype(int)\n",
    "        # Base case\n",
    "        if self.max_depth <= 0 or len(np.unique(y)) == 1:\n",
    "            self.pred = np.bincount(y).argmax()  # Find the most common value\n",
    "            return\n",
    "        \n",
    "        num_samples, num_features = X.shape\n",
    "        best_gain = 0\n",
    "        # GrowTree\n",
    "        for idx in range(num_features):\n",
    "            thresholds = np.unique(X[:, idx])\n",
    "            for thresh in thresholds:  # Repeatly call information_gain to validate the best split\n",
    "                gain = DecisionTree.information_gain(X, y, idx, thresh)\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    self.split_idx, self.thresh = idx, thresh\n",
    "\n",
    "        # If no improvement, make this a leaf node\n",
    "        if best_gain == 0:\n",
    "            self.pred = np.bincount(y).argmax()\n",
    "            return\n",
    "        # If found a useful split, proceed recursively\n",
    "        left_indices, right_indices = np.where(X[:, self.split_idx] < self.thresh)[0], np.where(X[:, self.split_idx] >= self.thresh)[0]\n",
    "        self.left, self.right = DecisionTree(self.max_depth - 1, self.features), DecisionTree(self.max_depth - 1, self.features)\n",
    "        # Recursively fit the left, right child\n",
    "        self.left.fit(X[left_indices], y[left_indices])\n",
    "        self.right.fit(X[right_indices], y[right_indices])\n",
    "\n",
    "    def predict(self, X):\n",
    "        # TODO\n",
    "        # Traverse down to the leaf node\n",
    "        def predict_once(sample, node):\n",
    "            if node.pred is not None:\n",
    "                return node.pred\n",
    "            # Recursive down the left children\n",
    "            if sample[node.split_idx] < node.thresh:\n",
    "                return predict_once(sample, node.left)\n",
    "            else:\n",
    "                return predict_once(sample, node.right)\n",
    "\n",
    "        predictions = [predict_once(sample, self) for sample in X]\n",
    "        return np.array(predictions)\n",
    "\n",
    "    def __repr__(self):\n",
    "        if self.max_depth == 0:\n",
    "            return \"%s (%s)\" % (self.pred, self.labels.size)\n",
    "        else:\n",
    "            return \"[%s < %s: %s | %s]\" % (self.features[self.split_idx],\n",
    "                                           self.thresh, self.left.__repr__(),\n",
    "                                           self.right.__repr__())\n",
    "\n",
    "# Output Model Accuracy\n",
    "spam_decisionTree_clf = DecisionTree(max_depth=3)\n",
    "spam_decisionTree_clf.fit(spam_training_data, spam_training_labels)\n",
    "spam_decisionTree_pred = spam_decisionTree_clf.predict(spam_training_data)\n",
    "print(f'Spam Decision Tree Accuracy: {evaluate_simple(spam_decisionTree_pred, spam_training_labels)}')\n",
    "\n",
    "titanic_decisionTree_clf = DecisionTree(max_depth=3)\n",
    "titanic_decisionTree_clf.fit(titanic_training, titanic_training_labels)\n",
    "titanic_decisionTree_pred = titanic_decisionTree_clf.predict(titanic_training)\n",
    "print(f'Titanic Decision Tree Accuracy: {evaluate_simple(titanic_decisionTree_pred, titanic_training_labels)}\\n')\n",
    "\n",
    "\n",
    "# Validation\n",
    "random.seed(246810)\n",
    "np.random.seed(246810)\n",
    "\n",
    "spam_train, spam_valid, spam_train_labels, spam_valid_labels = train_valid_split(spam_training_data, spam_training_labels, 0.2)\n",
    "tit_train, tit_valid, tit_train_labels, tit_valid_labels = train_valid_split(titanic_training, titanic_training_labels, 0.2)\n",
    "\n",
    "def decisionTree_eval(X_train, X_valid, y_train, y_valid):\n",
    "    clf = DecisionTree(max_depth=3)\n",
    "    clf.fit(X_train, y_train)\n",
    "    train_pred, valid_pred = clf.predict(X_train), clf.predict(X_valid)\n",
    "    train_accuracy, valid_accuracy = evaluate_simple(train_pred, y_train), evaluate_simple(valid_pred, y_valid)\n",
    "    return train_accuracy, valid_accuracy\n",
    "\n",
    "# Spam decisionTree\n",
    "spam_train_accuracy, spam_valid_accuracy = decisionTree_eval(spam_train, spam_valid, spam_train_labels, spam_valid_labels)\n",
    "print(f\"Spam training accuracy for decisionTree: {spam_train_accuracy},\\nSpam validation accuracy for decisionTree: {spam_valid_accuracy}\\n\")\n",
    "\n",
    "# Titanic decisionTree\n",
    "tit_train_accuracy, tit_valid_accuracy = decisionTree_eval(tit_train, tit_valid, tit_train_labels, tit_valid_labels)\n",
    "print(f\"Titanic training accuracy for decisionTree: {tit_train_accuracy},\\nTitanic validation accuracy for decisionTree: {tit_valid_accuracy}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef8a033b-ee13-4aca-98f6-4a0eb4fbd341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn:\n",
      "Spam Decision Tree Accuracy: 0.8008527269497246\n",
      "Titanic Decision Tree Accuracy: 0.8107036669970268\n",
      "\n",
      "Spam training accuracy for decisionTree: 0.8071111111111111,\n",
      "Spam validation accuracy for decisionTree: 0.7795293072824157\n",
      "\n",
      "Titanic training accuracy for decisionTree: 0.8308457711442786,\n",
      "Titanic validation accuracy for decisionTree: 0.7834158415841584\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sklearn decision Tree accuracy\n",
    "random.seed(246810)\n",
    "np.random.seed(246810)\n",
    "\n",
    "clf = DecisionTreeClassifier(max_depth=3)\n",
    "clf.fit(spam_training_data, spam_training_labels)\n",
    "new_pred = clf.predict(spam_training_data)\n",
    "print(f'Sklearn:\\nSpam Decision Tree Accuracy: {evaluate_simple(new_pred, spam_training_labels)}')\n",
    "\n",
    "tit_clf = DecisionTreeClassifier(max_depth=3)\n",
    "tit_clf.fit(titanic_training, titanic_training_labels)\n",
    "new_tit_pred = tit_clf.predict(titanic_training)\n",
    "print(f'Titanic Decision Tree Accuracy: {evaluate_simple(new_tit_pred, titanic_training_labels)}\\n')\n",
    "\n",
    "# sklearn decisionTree validation accuracy\n",
    "spam_train, spam_valid, spam_train_labels, spam_valid_labels = train_valid_split(spam_training_data, spam_training_labels, 0.2)\n",
    "tit_train, tit_valid, tit_train_labels, tit_valid_labels = train_valid_split(titanic_training, titanic_training_labels, 0.2)\n",
    "\n",
    "def decisionTree_eval(X_train, X_valid, y_train, y_valid):\n",
    "    clf = DecisionTreeClassifier(max_depth=3)\n",
    "    clf.fit(X_train, y_train)\n",
    "    train_pred, valid_pred = clf.predict(X_train), clf.predict(X_valid)\n",
    "    train_accuracy, valid_accuracy = evaluate_simple(train_pred, y_train), evaluate_simple(valid_pred, y_valid)\n",
    "    return train_accuracy, valid_accuracy\n",
    "\n",
    "# Spam decisionTree\n",
    "spam_train_accuracy, spam_valid_accuracy = decisionTree_eval(spam_train, spam_valid, spam_train_labels, spam_valid_labels)\n",
    "print(f\"Spam training accuracy for decisionTree: {spam_train_accuracy},\\nSpam validation accuracy for decisionTree: {spam_valid_accuracy}\\n\")\n",
    "\n",
    "# Titanic decisionTree\n",
    "tit_train_accuracy, tit_valid_accuracy = decisionTree_eval(tit_train, tit_valid, tit_train_labels, tit_valid_labels)\n",
    "print(f\"Titanic training accuracy for decisionTree: {tit_train_accuracy},\\nTitanic validation accuracy for decisionTree: {tit_valid_accuracy}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6e19cc09-276d-451c-b5b9-424a29e9f37b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn bagging:\n",
      "Spam bagging Training Accuracy: 0.8037\n",
      "Titanic baggedTree Training Accuracy: 0.81863\n",
      "\n",
      "Spam training accuracy for bagging: 0.83822,\n",
      "Spam validation accuracy for bagging: 0.80617\n",
      "\n",
      "Titanic training accuracy for bagging: 0.85075,\n",
      "Titanic validation accuracy for bagging: 0.78465\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sklearn BaggedTrees\n",
    "# Validation on n2\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "random.seed(246810)\n",
    "np.random.seed(246810)\n",
    "\n",
    "baggedTree_clf = BaggingClassifier(base_estimator=clf, n_estimators=200)\n",
    "baggedTree_clf.fit(spam_training_data, spam_training_labels)\n",
    "baggedTree_pred = baggedTree_clf.predict(spam_training_data)\n",
    "print(f\"Sklearn bagging:\\nSpam bagging Training Accuracy: {round(evaluate_simple(baggedTree_pred, spam_training_labels), 5)}\")\n",
    "\n",
    "tit_bagging_clf = BaggingClassifier(base_estimator=clf, n_estimators=200)\n",
    "tit_bagging_clf.fit(titanic_training, titanic_training_labels)\n",
    "tit_bagging_predictions = tit_bagging_clf.predict(titanic_training)\n",
    "print(f\"Titanic baggedTree Training Accuracy: {round(evaluate_simple(tit_bagging_predictions, titanic_training_labels), 5)}\\n\")\n",
    "\n",
    "\n",
    "# sklearn baggedTrees validation accuracy\n",
    "spam_train, spam_valid, spam_train_labels, spam_valid_labels = train_valid_split(spam_training_data, spam_training_labels, 0.2)\n",
    "tit_train, tit_valid, tit_train_labels, tit_valid_labels = train_valid_split(titanic_training, titanic_training_labels, 0.2)\n",
    "\n",
    "def bagging_eval(X_train, X_valid, y_train, y_valid):\n",
    "    clf = BaggingClassifier(base_estimator=DecisionTreeClassifier(max_depth=3), n_estimators=200)\n",
    "    clf.fit(X_train, y_train)\n",
    "    train_pred, valid_pred = clf.predict(X_train), clf.predict(X_valid)\n",
    "    train_accuracy, valid_accuracy = evaluate_simple(train_pred, y_train), evaluate_simple(valid_pred, y_valid)\n",
    "    return train_accuracy, valid_accuracy\n",
    "\n",
    "# Spam baggedTrees\n",
    "spam_train_accuracy, spam_valid_accuracy = bagging_eval(spam_train, spam_valid, spam_train_labels, spam_valid_labels)\n",
    "print(f\"Spam training accuracy for bagging: {round(spam_train_accuracy, 5)},\\nSpam validation accuracy for bagging: {round(spam_valid_accuracy, 5)}\\n\")\n",
    "\n",
    "# Titanic baggedTrees\n",
    "tit_train_accuracy, tit_valid_accuracy = bagging_eval(tit_train, tit_valid, tit_train_labels, tit_valid_labels)\n",
    "print(f\"Titanic training accuracy for bagging: {round(tit_train_accuracy, 5)},\\nTitanic validation accuracy for bagging: {round(tit_valid_accuracy, 5)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a6d0ae-6f23-4dbe-9ef3-bf12466d35b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sklearn RandomForest\n",
    "# rf spam needs higher depth, titanic needs less depth\n",
    "random.seed(246810)\n",
    "np.random.seed(246810)\n",
    "\n",
    "spam_rf_clf = RandomForestClassifier(n_estimators=200, max_depth=8)\n",
    "spam_rf_clf.fit(spam_training_data, spam_training_labels)\n",
    "spam_rf_pred = spam_rf_clf.predict(spam_training_data)\n",
    "print(f\"Sklearn randomForest:\\nSpam randomForest Training Accuracy: {evaluate_simple(spam_rf_pred, spam_training_labels)}\")\n",
    "\n",
    "tit_rf_clf = RandomForestClassifier(n_estimators=200, max_depth=8)\n",
    "tit_rf_clf.fit(titanic_training, titanic_training_labels)\n",
    "tit_rf_predictions = tit_rf_clf.predict(titanic_training)\n",
    "print(f\"Titanic randomForest Training Accuracy: {evaluate_simple(tit_rf_predictions, titanic_training_labels)}\\n\")\n",
    "\n",
    "\n",
    "# sklearn random Forest validation accuracy\n",
    "spam_train, spam_valid, spam_train_labels, spam_valid_labels = train_valid_split(spam_training_data, spam_training_labels, 0.2)\n",
    "tit_train, tit_valid, tit_train_labels, tit_valid_labels = train_valid_split(titanic_training, titanic_training_labels, 0.2)\n",
    "\n",
    "def rf_eval(X_train, X_valid, y_train, y_valid):\n",
    "    clf = RandomForestClassifier(n_estimators=200, max_depth=8)\n",
    "    clf.fit(X_train, y_train)\n",
    "    train_pred, valid_pred = clf.predict(X_train), clf.predict(X_valid)\n",
    "    train_accuracy, valid_accuracy = evaluate_simple(train_pred, y_train), evaluate_simple(valid_pred, y_valid)\n",
    "    return train_accuracy, valid_accuracy\n",
    "\n",
    "# Spam RandomForest\n",
    "spam_train_accuracy, spam_valid_accuracy = rf_eval(spam_train, spam_valid, spam_train_labels, spam_valid_labels)\n",
    "print(f\"Spam training accuracy for randomForest: {spam_train_accuracy},\\nSpam validation accuracy for randomForest: {spam_valid_accuracy}\\n\")\n",
    "\n",
    "# Titanic RandomForest\n",
    "tit_train_accuracy, tit_valid_accuracy = rf_eval(tit_train, tit_valid, tit_train_labels, tit_valid_labels)\n",
    "print(f\"Titanic training accuracy for randomForest: {tit_train_accuracy},\\nTitanic validation accuracy for randomForest: {tit_valid_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9b9363f2-e2a4-4aa4-b76b-902b681fd4a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spam ensemble learning Accuracy: 0.8068383658969804\n",
      "Titanic ensemble learning Accuracy: 0.7933168316831684\n"
     ]
    }
   ],
   "source": [
    "# Sklearn ensemble learning\n",
    "\n",
    "#create a dictionary of our models\n",
    "estimators=[('dt', clf), ('bag', baggedTree_clf), ('rf', spam_rf_clf)]\n",
    "\n",
    "ensemble = VotingClassifier(estimators, voting='hard')\n",
    "ensemble.fit(spam_train, spam_train_labels)\n",
    "print(f\"Spam ensemble learning Accuracy: {ensemble.score(spam_valid, spam_valid_labels)}\")\n",
    "\n",
    "\n",
    "# Titanic\n",
    "tit_estimators=[('tdt', tit_clf), ('tbag', tit_bagging_clf), ('trf', tit_rf_clf)]\n",
    "tit_ensemble = VotingClassifier(tit_estimators, voting='hard')\n",
    "tit_ensemble.fit(tit_train, tit_train_labels)\n",
    "print(f\"Titanic ensemble learning Accuracy: {tit_ensemble.score(tit_valid, tit_valid_labels)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
